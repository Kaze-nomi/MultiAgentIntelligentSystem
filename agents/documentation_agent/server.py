"""
Documentation Agent - –°–æ–∑–¥–∞—ë—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∫–æ–¥–∞

–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏:
1. –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞–∫—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å
2. –ê–Ω–∞–ª–∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ —Å—Ç–∏–ª—è
3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ README
4. –°–æ–∑–¥–∞–Ω–∏–µ API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∫–æ–¥–∞ (docstrings reference)
6. –°–æ–∑–¥–∞–Ω–∏–µ CHANGELOG entries
7. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è architecture documentation
8. –°–æ–∑–¥–∞–Ω–∏–µ user/developer guides

–ü–æ–ª—É—á–∞–µ—Ç:
- –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–¥ –æ—Ç Code Writer
- –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –æ—Ç Architect
- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–µ–≤—å—é –æ—Ç Code Reviewer
- –ö–æ–Ω—Ç–µ–∫—Å—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
- –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
- –§–∞–π–ª—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ (README, API docs, etc.)
- CHANGELOG entries
- Module documentation
"""

import os
import json
import logging
import re
import time
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from contextlib import asynccontextmanager

import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
import httpx

from models import (
    DocType, DocFormat, DocLanguage, ChangeType,
    DocStyle, DocFile,
    ChangelogEntry, ChangelogVersion,
    ApiParameter, ApiResponse, ApiEndpoint, ApiDocumentation,
    FunctionDoc, ClassDoc, ModuleDoc,
    CodeFileInput, ArchitectureInput, ReviewInput,
    DocumentationRequest, DocumentationResponse,
    TechStack
)

from logging_config import setup_logging

# ============================================================================
# CONFIGURATION
# ============================================================================

logger = setup_logging("documentation")

OPENROUTER_MCP_URL = os.getenv("OPENROUTER_MCP_URL", "http://openrouter-mcp:8000")
LLM_TIMEOUT = 1000
DEFAULT_MODEL = os.getenv("DEFAULT_MODEL")

# ============================================================================
# HTTP CLIENT
# ============================================================================

http_client: Optional[httpx.AsyncClient] = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifecycle manager"""
    global http_client
    http_client = httpx.AsyncClient(timeout=httpx.Timeout(LLM_TIMEOUT))
    logger.info("Documentation Agent started")
    yield
    await http_client.aclose()
    logger.info("Documentation Agent stopped")


# ============================================================================
# FASTAPI APP
# ============================================================================

app = FastAPI(
    title="Documentation Agent",
    description="–ê–≥–µ–Ω—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–¥–∞ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã",
    version="2.1.0",
    lifespan=lifespan
)


# ============================================================================
# LLM HELPER
# ============================================================================

async def call_llm(
    prompt: str,
    system_prompt: Optional[str] = None,
    temperature: float = 0.3,
    max_tokens: int = 100000,
    step: str = "documentation_llm_request"
) -> str:
    """–í—ã–∑–æ–≤ LLM —á–µ—Ä–µ–∑ OpenRouter MCP"""

    if not system_prompt:
        system_prompt = """–¢—ã –æ–ø—ã—Ç–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –ø–∏—Å–∞—Ç–µ–ª—å —Å 15+ –ª–µ—Ç –æ–ø—ã—Ç–∞.
–¢—ã —Å–æ–∑–¥–∞—ë—à—å:
- –ü–æ–Ω—è—Ç–Ω—É—é, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
- –ü–æ–ª–µ–∑–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞
- –ß—ë—Ç–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
- –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ API reference

–¢—ã —É—á–∏—Ç—ã–≤–∞–µ—à—å:
- –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Å—Ç–∏–ª—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞
- –¶–µ–ª–µ–≤—É—é –∞—É–¥–∏—Ç–æ—Ä–∏—é
- –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

–í–ê–ñ–ù–û:
- –¢—ã –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–µ—à—å, —Ç–æ —á–µ–≥–æ –Ω–µ –±—ã–ª–æ –∫–æ–¥–µ –∏–ª–∏ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ
- –í—Å–µ–≥–¥–∞ –ø–∏—à–µ—à—å —Ç–æ–ª—å–∫–æ –æ —Ç–æ–º, —á—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ
- –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –≤—Ä–∏

–í–æ–∑–≤—Ä–∞—â–∞–µ—à—å –æ—Ç–≤–µ—Ç—ã –≤ Markdown –∏–ª–∏ JSON –∫–æ–≥–¥–∞ —ç—Ç–æ —É–∫–∞–∑–∞–Ω–æ."""

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt}
    ]

    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª–∞ –∑–∞–ø—Ä–æ—Å–∞ - —Ç–æ–ª—å–∫–æ —à–∞–≥
    logger.info(f"step: {step}")

    start_time = time.time()

    try:
        response = await http_client.post(
            f"{OPENROUTER_MCP_URL}/chat/completions",
            json={
                "model": DEFAULT_MODEL,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens
            },
            timeout=LLM_TIMEOUT
        )

        duration = time.time() - start_time

        if response.status_code == 200:
            response_data = response.json()
            content = response_data["choices"][0]["message"]["content"]

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–∫–µ–Ω–∞—Ö
            usage = response_data.get("usage", {})
            prompt_tokens = usage.get("prompt_tokens", 0)
            completion_tokens = usage.get("completion_tokens", 0)
            total_tokens = usage.get("total_tokens", 0)

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ reasoning (–µ—Å–ª–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç)
            reasoning = None
            if "reasoning" in response_data["choices"][0]["message"]:
                reasoning = response_data["choices"][0]["message"]["reasoning"]
            elif "reasoning_content" in response_data["choices"][0]["message"]:
                reasoning = response_data["choices"][0]["message"]["reasoning_content"]

            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
            response_log = {
                "event": "llm_request_success",
                "step": step,
                "model": DEFAULT_MODEL,
                "duration_seconds": round(duration, 3),
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": total_tokens,
                "content": content,
                "reasoning": reasoning,
                "timestamp": datetime.now().isoformat()
            }
            logger.info(json.dumps(response_log, ensure_ascii=False))

            return content
        else:
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–∫–∏
            error_log = {
                "event": "llm_request_error",
                "model": DEFAULT_MODEL,
                "duration_seconds": round(duration, 3),
                "status_code": response.status_code,
                "error_response": response.text,
                "timestamp": datetime.now().isoformat()
            }
            logger.error(json.dumps(error_log, ensure_ascii=False))
            return ""

    except Exception as e:
        duration = time.time() - start_time
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        exception_log = {
            "event": "llm_request_exception",
            "step": step,
            "model": DEFAULT_MODEL,
            "duration_seconds": round(duration, 3),
            "exception": str(e),
            "timestamp": datetime.now().isoformat()
        }
        logger.error(json.dumps(exception_log, ensure_ascii=False))
        return ""


def parse_json_response(response: str) -> Optional[Dict]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
    try:
        return json.loads(response)
    except json.JSONDecodeError:
        pass
    
    try:
        json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(1))
        
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            return json.loads(json_match.group())
    except json.JSONDecodeError as e:
        logger.error(f"JSON parse error: {e}")
    
    return None


def count_words(text: str) -> int:
    """–°—á–∏—Ç–∞–µ—Ç —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ"""
    return len(text.split())


# ============================================================================
# AUTO-DETERMINE DOC TYPES
# ============================================================================

async def determine_doc_types(
    task: str,
    code_files: List[Dict[str, Any]],
    architecture: Dict[str, Any],
    review_result: Dict[str, Any],
    repo_context: Dict[str, Any],
    tech_stack: TechStack
) -> List[DocType]:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞–∫–∏–µ —Ç–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å
    –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
    
    Returns:
        List[DocType]: –°–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    """
    
    doc_types = []
    key_files = repo_context.get("key_files", {})
    
    # 1. README - –í–°–ï–ì–î–ê –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º/–æ–±–Ω–æ–≤–ª—è–µ–º
    doc_types.append(DocType.README)
    logger.info("  ‚Üí README: –í–ö–õ–Æ–ß–Å–ù (–≤—Å–µ–≥–¥–∞)")
    
    # 2. API Documentation - –µ—Å–ª–∏ –µ—Å—Ç—å API endpoints –≤ –∫–æ–¥–µ
    has_api = False
    api_indicators = ["@app.", "@router.", "def get", "def post", "def put", "def delete",
                      "async def", "route", "endpoint", "api", "controller", "FastAPI", "Flask", "Express"]
    
    for f in code_files:
        content = f.get("content", "").lower()
        path = f.get("path", "").lower()
        
        if any(indicator.lower() in content or indicator.lower() in path for indicator in api_indicators):
            has_api = True
            break
    
    if has_api:
        doc_types.append(DocType.API)
        logger.info("  ‚Üí API docs: –í–ö–õ–Æ–ß–Å–ù (–æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã API endpoints)")
    else:
        logger.info("  ‚Üí API docs: –ü–†–û–ü–£–©–ï–ù (–Ω–µ—Ç API endpoints)")
    
    # 3. Architecture Documentation - –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞
    if architecture and (architecture.get("components") or architecture.get("patterns")):
        doc_types.append(DocType.ARCHITECTURE)
        logger.info(f"  ‚Üí Architecture: –í–ö–õ–Æ–ß–Å–ù (–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {len(architecture.get('components', []))})")
    else:
        logger.info("  ‚Üí Architecture: –ü–†–û–ü–£–©–ï–ù (–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã)")
    
    # 4. CHANGELOG - –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–≤—å—é –∏–ª–∏ —ç—Ç–æ –Ω–æ–≤–∞—è —Ñ–∏—á–∞
    task_lower = task.lower()
    is_feature = any(word in task_lower for word in ["–¥–æ–±–∞–≤–∏—Ç—å", "—Å–æ–∑–¥–∞—Ç—å", "—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å", "add", "create", "implement", "feature"])
    is_fix = any(word in task_lower for word in ["–∏—Å–ø—Ä–∞–≤–∏—Ç—å", "fix", "bug", "–æ—à–∏–±–∫–∞", "–±–∞–≥"])
    
    if review_result or is_feature or is_fix:
        doc_types.append(DocType.CHANGELOG)
        logger.info(f"  ‚Üí CHANGELOG: –í–ö–õ–Æ–ß–Å–ù (feature={is_feature}, fix={is_fix}, has_review={bool(review_result)})")
    else:
        logger.info("  ‚Üí CHANGELOG: –ü–†–û–ü–£–©–ï–ù")
    
    # 5. Code Reference - –µ—Å–ª–∏ –º–Ω–æ–≥–æ –∫–æ–¥–∞ (–±–æ–ª–µ–µ 3 —Ñ–∞–π–ª–æ–≤) –∏–ª–∏ –µ—Å—Ç—å –∫–ª–∞—Å—Å—ã
    has_classes = False
    for f in code_files:
        content = f.get("content", "")
        if "class " in content:
            has_classes = True
            break
    
    if len(code_files) >= 3 or has_classes:
        doc_types.append(DocType.CODE)
        logger.info(f"  ‚Üí Code Reference: –í–ö–õ–Æ–ß–Å–ù (—Ñ–∞–π–ª–æ–≤: {len(code_files)}, –∫–ª–∞—Å—Å—ã: {has_classes})")
    else:
        logger.info(f"  ‚Üí Code Reference: –ü–†–û–ü–£–©–ï–ù (—Ñ–∞–π–ª–æ–≤: {len(code_files)}, –º–∞–ª–æ –¥–ª—è reference)")
    
    # 6. CONTRIBUTING - —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –Ω–æ–≤—ã–π –ø—Ä–æ–µ–∫—Ç (–Ω–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ CONTRIBUTING)
    has_contributing = any("contributing" in k.lower() for k in key_files.keys())
    
    if not has_contributing and len(code_files) >= 5:
        doc_types.append(DocType.CONTRIBUTING)
        logger.info("  ‚Üí CONTRIBUTING: –í–ö–õ–Æ–ß–Å–ù (–Ω–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ, –ø—Ä–æ–µ–∫—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π)")
    else:
        logger.info(f"  ‚Üí CONTRIBUTING: –ü–†–û–ü–£–©–ï–ù (exists={has_contributing})")
    
    logger.info(f"–ò—Ç–æ–≥–æ —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏: {len(doc_types)} - {[dt.value for dt in doc_types]}")
    
    return doc_types


# ============================================================================
# DOCUMENTATION STYLE ANALYSIS
# ============================================================================

async def analyze_doc_style(repo_context: Dict[str, Any]) -> DocStyle:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Å—Ç–∏–ª—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞
    """
    
    key_files = repo_context.get("key_files", {})
    
    # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
    doc_files = {}
    for path, content in key_files.items():
        lower_path = path.lower()
        if any(x in lower_path for x in ["readme", "doc", "guide", "changelog", "contributing"]):
            doc_files[path] = content[:3000] if content else ""
    
    if not doc_files:
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–∏–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        return DocStyle()
    
    prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å—Ç–∏–ª—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤ –ø—Ä–æ–µ–∫—Ç–µ.

## –°–£–©–ï–°–¢–í–£–Æ–©–ê–Ø –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–Ø:
{json.dumps(doc_files, indent=2, ensure_ascii=False)}

## –û–ü–†–ï–î–ï–õ–ò:

1. –§–æ—Ä–º–∞—Ç: markdown, rst, asciidoc
2. –Ø–∑—ã–∫: ru, en
3. –°—Ç–∏–ª—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: atx (#) –∏–ª–∏ setext
4. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ª–∏ badges
5. –ï—Å—Ç—å –ª–∏ Table of Contents
6. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ª–∏ emoji
7. –ö–∞–∫–∏–µ —Å–µ–∫—Ü–∏–∏ –µ—Å—Ç—å –≤ README

## –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (JSON):
{{
    "format": "markdown",
    "language": "ru",
    "heading_style": "atx",
    "code_fence": "```",
    "list_marker": "-",
    "use_badges": true,
    "use_toc": true,
    "use_emojis": true,
    "readme_sections": ["description", "installation", "usage", "api", "license"]
}}
"""
    
    response = await call_llm(prompt, step="doc_style_analysis")
    parsed = parse_json_response(response)
    
    if parsed:
        try:
            doc_format = DocFormat(parsed.get("format", "markdown"))
        except ValueError:
            doc_format = DocFormat.MARKDOWN
        
        try:
            doc_lang = DocLanguage(parsed.get("language", "ru"))
        except ValueError:
            doc_lang = DocLanguage.RUSSIAN
        
        return DocStyle(
            format=doc_format,
            language=doc_lang,
            heading_style=parsed.get("heading_style", "atx"),
            code_fence=parsed.get("code_fence", "```"),
            list_marker=parsed.get("list_marker", "-"),
            use_badges=parsed.get("use_badges", True),
            use_toc=parsed.get("use_toc", True),
            use_emojis=parsed.get("use_emojis", True),
            readme_sections=parsed.get("readme_sections", [])
        )
    
    return DocStyle()


# ============================================================================
# README GENERATION
# ============================================================================

async def generate_readme(
    task: str,
    code_files: List[Dict[str, Any]],
    architecture: Dict[str, Any],
    tech_stack: TechStack,
    doc_style: DocStyle,
    existing_readme: Optional[str] = None
) -> DocFile:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç README
    """
    
    # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–¥–µ
    code_summary = []
    for f in code_files[:10]:
        code_summary.append({
            "path": f.get("path", ""),
            "description": f.get("description", ""),
            "language": f.get("language", "")
        })
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ
    components = architecture.get("components", [])[:10] if architecture else []
    patterns = architecture.get("patterns", []) if architecture else []
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —Å–µ–∫—Ü–∏–π
    sections = doc_style.readme_sections or [
        "description", "features", "installation", 
        "quick_start", "usage", "api", "configuration",
        "testing", "contributing", "license"
    ]
    
    existing_section = ""
    if existing_readme:
        existing_section = f"""
## –°–£–©–ï–°–¢–í–£–Æ–©–ò–ô README (–æ–±–Ω–æ–≤–∏ –µ–≥–æ):
{existing_readme[:5000]}
"""
    
    emoji_note = "–ò—Å–ø–æ–ª—å–∑—É–π emoji –¥–ª—è —Å–µ–∫—Ü–∏–π (üì¶, üöÄ, ‚öôÔ∏è, etc.)" if doc_style.use_emojis else "–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π emoji"
    toc_note = "–î–æ–±–∞–≤—å Table of Contents –≤ –Ω–∞—á–∞–ª–æ" if doc_style.use_toc else ""
    badge_note = "–î–æ–±–∞–≤—å badges (build status, version, license)" if doc_style.use_badges else ""
    
    prompt = f"""
–°–æ–∑–¥–∞–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π README.md –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞.

## –ó–ê–î–ê–ß–ê (—á—Ç–æ –±—ã–ª–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ):
{task}

## –ù–û–í–´–ô –ö–û–î:
{json.dumps(code_summary, indent=2, ensure_ascii=False)}

## –ê–†–•–ò–¢–ï–ö–¢–£–†–ê:
- –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã: {json.dumps([c.get("name") if isinstance(c, dict) else c for c in components], ensure_ascii=False)}
- –ü–∞—Ç—Ç–µ—Ä–Ω—ã: {', '.join(patterns) if patterns else 'N/A'}

## –¢–ï–•–ù–û–õ–û–ì–ò–ò:
- –Ø–∑—ã–∫: {tech_stack.primary_language}
- –§—Ä–µ–π–º–≤–æ—Ä–∫–∏: {', '.join(tech_stack.frameworks) if tech_stack.frameworks else 'N/A'}
- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: {', '.join(tech_stack.testing_frameworks) if tech_stack.testing_frameworks else 'N/A'}
- –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: {', '.join(tech_stack.tools) if tech_stack.tools else 'N/A'}

## –°–¢–ò–õ–¨ –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–ò:
- –Ø–∑—ã–∫: {'–†—É—Å—Å–∫–∏–π' if doc_style.language == DocLanguage.RUSSIAN else 'English'}
- {emoji_note}
- {toc_note}
- {badge_note}

## –°–ï–ö–¶–ò–ò –î–õ–Ø –í–ö–õ–Æ–ß–ï–ù–ò–Ø:
{', '.join(sections)}
{existing_section}

## –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
1. –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, –ø–æ–Ω—è—Ç–Ω—ã–π —Å—Ç–∏–ª—å
2. –ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ —Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞
3. –ß—ë—Ç–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ —É—Å—Ç–∞–Ω–æ–≤–∫–µ
4. –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
5. –ï—Å–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ—à—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π - —Å–æ—Ö—Ä–∞–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –¥–æ–±–∞–≤—å –Ω–æ–≤–æ–µ

–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ Markdown –∫–æ–Ω—Ç–µ–Ω—Ç, –±–µ–∑ ```markdown –±–ª–æ–∫–æ–≤.
"""
    
    content = await call_llm(prompt, max_tokens=100000, step="readme_generation")
    
    # –û—á–∏—â–∞–µ–º –æ—Ç markdown –±–ª–æ–∫–æ–≤
    content = re.sub(r'^```(?:markdown)?\n?', '', content)
    content = re.sub(r'\n?```$', '', content)
    content = content.strip()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
    action = "update" if existing_readme else "create"
    
    return DocFile(
        path="README.md",
        content=content,
        doc_type=DocType.README,
        format=doc_style.format,
        description="Project README",
        action=action,
        word_count=count_words(content)
    )


# ============================================================================
# API DOCUMENTATION
# ============================================================================

async def extract_api_endpoints(
    code_files: List[Dict[str, Any]],
    tech_stack: TechStack
) -> List[ApiEndpoint]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç API endpoints –∏–∑ –∫–æ–¥–∞
    """
    
    # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–¥ —Å API
    api_code = []
    for f in code_files:
        path = f.get("path", "").lower()
        content = f.get("content", "")
        
        # –ò—â–µ–º —Ñ–∞–π–ª—ã —Å API
        if any(x in path for x in ["route", "api", "endpoint", "controller", "view"]):
            api_code.append({"path": f.get("path"), "content": content[:4000]})
        elif any(x in content.lower() for x in ["@app.", "@router.", "def get", "def post", "async def", "route", "endpoint", "api", "controller", "FastAPI", "Flask", "Express"]):
            api_code.append({"path": f.get("path"), "content": content[:4000]})
    
    if not api_code:
        return []
    
    prompt = f"""
–ò–∑–≤–ª–µ–∫–∏ API endpoints –∏–∑ –∫–æ–¥–∞.

## –§–†–ï–ô–ú–í–û–†–ö–ò:
{', '.join(tech_stack.frameworks) if tech_stack.frameworks else 'Unknown'}

## –ö–û–î:
{json.dumps(api_code, indent=2, ensure_ascii=False)}

## –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (JSON):
{{
    "endpoints": [
        {{
            "method": "POST",
            "path": "/api/auth/login",
            "summary": "–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è",
            "description": "–ê–≤—Ç–æ—Ä–∏–∑—É–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç JWT —Ç–æ–∫–µ–Ω",
            "tags": ["auth"],
            "parameters": [
                {{
                    "name": "username",
                    "type": "string",
                    "required": true,
                    "description": "–ò–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è",
                    "location": "body"
                }}
            ],
            "request_body": {{
                "content_type": "application/json",
                "example": {{"username": "user", "password": "pass"}}
            }},
            "responses": [
                {{
                    "status_code": 200,
                    "description": "–£—Å–ø–µ—à–Ω–∞—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è",
                    "example": {{"access_token": "...", "token_type": "bearer"}}
                }},
                {{
                    "status_code": 401,
                    "description": "–ù–µ–≤–µ—Ä–Ω—ã–µ credentials"
                }}
            ],
            "authentication": "None"
        }}
    ]
}}
"""
    
    response = await call_llm(prompt, step="api_endpoints_extraction")
    parsed = parse_json_response(response)
    
    endpoints = []
    
    if parsed:
        for ep_data in parsed.get("endpoints", []):
            parameters = []
            for param in ep_data.get("parameters", []):
                parameters.append(ApiParameter(**param))
            
            responses = []
            for resp in ep_data.get("responses", []):
                responses.append(ApiResponse(**resp))
            
            endpoints.append(ApiEndpoint(
                method=ep_data.get("method", "GET"),
                path=ep_data.get("path", ""),
                summary=ep_data.get("summary", ""),
                description=ep_data.get("description", ""),
                tags=ep_data.get("tags", []),
                parameters=parameters,
                request_body=ep_data.get("request_body"),
                responses=responses,
                authentication=ep_data.get("authentication")
            ))
    
    return endpoints


async def generate_api_documentation(
    endpoints: List[ApiEndpoint],
    tech_stack: TechStack,
    doc_style: DocStyle
) -> Optional[DocFile]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
    """
    
    if not endpoints:
        logger.info("No API endpoints found, skipping API documentation")
        return None
    
    endpoints_info = [ep.dict() for ep in endpoints]
    
    lang_note = "–ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ" if doc_style.language == DocLanguage.RUSSIAN else "Write in English"
    
    prompt = f"""
–°–æ–∑–¥–∞–π –ø–æ–ª–Ω—É—é API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –≤ Markdown.

## ENDPOINTS:
{json.dumps(endpoints_info, indent=2, ensure_ascii=False)}

## –§–†–ï–ô–ú–í–û–†–ö:
{', '.join(tech_stack.frameworks) if tech_stack.frameworks else 'Unknown'}

## –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
1. {lang_note}
2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ endpoint:
   - –û–ø–∏—Å–∞–Ω–∏–µ
   - HTTP –º–µ—Ç–æ–¥ –∏ –ø—É—Ç—å
   - –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å —Ç–∏–ø–∞–º–∏
   - Request body (–µ—Å–ª–∏ –µ—Å—Ç—å)
   - Responses —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
   - –ü—Ä–∏–º–µ—Ä—ã curl/httpie
3. –ì—Ä—É–ø–ø–∏—Ä—É–π –ø–æ —Ç–µ–≥–∞–º/—Ä–µ—Å—É—Ä—Å–∞–º
4. –î–æ–±–∞–≤—å Table of Contents
5. –î–æ–±–∞–≤—å —Å–µ–∫—Ü–∏—é Authentication –µ—Å–ª–∏ –Ω—É–∂–Ω–æ

## –§–û–†–ú–ê–¢:
Markdown —Å code blocks –¥–ª—è –ø—Ä–∏–º–µ—Ä–æ–≤.

–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ Markdown –∫–æ–Ω—Ç–µ–Ω—Ç.
"""
    
    content = await call_llm(prompt, max_tokens=100000, step="api_docs_generation")
    
    content = re.sub(r'^```(?:markdown)?\n?', '', content)
    content = re.sub(r'\n?```$', '', content)
    content = content.strip()
    
    return DocFile(
        path="docs/api.md",
        content=content,
        doc_type=DocType.API,
        format=doc_style.format,
        description="API Documentation",
        action="create",
        word_count=count_words(content)
    )


# ============================================================================
# CODE DOCUMENTATION
# ============================================================================

async def generate_code_documentation(
    code_files: List[Dict[str, Any]],
    architecture: Dict[str, Any],
    tech_stack: TechStack,
    doc_style: DocStyle
) -> DocFile:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –∫–æ–¥–∞ (module reference)
    """
    
    # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞—Ö
    components = architecture.get("components", []) if architecture else []
    interfaces = architecture.get("interfaces", []) if architecture else []
    
    # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–¥–∞
    code_info = []
    for f in code_files[:50]:
        code_info.append({
            "path": f.get("path", ""),
            "description": f.get("description", ""),
            "classes": f.get("classes", []),
            "functions": f.get("functions", []),
            "content_preview": f.get("content", "")[:15000]
        })
    
    lang_note = "–ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ" if doc_style.language == DocLanguage.RUSSIAN else "Write in English"
    
    prompt = f"""
–°–æ–∑–¥–∞–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –∫–æ–¥–∞ (Code Reference) –≤ Markdown.

## –ê–†–•–ò–¢–ï–ö–¢–£–†–ê:
### –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
{json.dumps(components[:10], indent=2, ensure_ascii=False)}

### –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã:
{json.dumps(interfaces[:5], indent=2, ensure_ascii=False)}

## –ö–û–î:
{json.dumps(code_info, indent=2, ensure_ascii=False)}

## –¢–ï–•–ù–û–õ–û–ì–ò–ò:
- –Ø–∑—ã–∫: {tech_stack.primary_language}

## –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
1. {lang_note}
2. –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π –∫–∞–∂–¥—ã–π –º–æ–¥—É–ª—å:
   - –û–ø–∏—Å–∞–Ω–∏–µ –º–æ–¥—É–ª—è
   - –ö–ª–∞—Å—Å—ã —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–æ–≤
   - –§—É–Ω–∫—Ü–∏–∏ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
   - –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
3. –ì—Ä—É–ø–ø–∏—Ä—É–π –ø–æ –º–æ–¥—É–ª—è–º/–ø–∞–∫–µ—Ç–∞–º
4. –î–æ–±–∞–≤—å Table of Contents
5. –ò—Å–ø–æ–ª—å–∑—É–π code blocks –¥–ª—è –ø—Ä–∏–º–µ—Ä–æ–≤

–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ Markdown –∫–æ–Ω—Ç–µ–Ω—Ç.
"""
    
    content = await call_llm(prompt, max_tokens=100000, step="code_docs_generation")
    
    content = re.sub(r'^```(?:markdown)?\n?', '', content)
    content = re.sub(r'\n?```$', '', content)
    content = content.strip()
    
    return DocFile(
        path="docs/code-reference.md",
        content=content,
        doc_type=DocType.CODE,
        format=doc_style.format,
        description="Code Reference Documentation",
        action="create",
        word_count=count_words(content)
    )


# ============================================================================
# ARCHITECTURE DOCUMENTATION
# ============================================================================

async def generate_architecture_documentation(
    architecture: Dict[str, Any],
    tech_stack: TechStack,
    doc_style: DocStyle
) -> Optional[DocFile]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
    """
    
    if not architecture:
        logger.info("No architecture data, skipping architecture documentation")
        return None
    
    components = architecture.get("components", [])
    patterns = architecture.get("patterns", [])
    file_structure = architecture.get("file_structure", [])
    diagrams = architecture.get("diagrams", {})
    recommendations = architecture.get("recommendations", [])
    integration_points = architecture.get("integration_points", [])
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–µ–∫—Ü–∏—é –¥–∏–∞–≥—Ä–∞–º–º
    diagrams_section = ""
    if diagrams:
        for diagram_type, plantuml_code in diagrams.items():
            diagrams_section += f"""
        ### {diagram_type.replace('_', ' ').title()} Diagram

        ```plantuml
        {plantuml_code}
        """

        lang_note = "–ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ" if doc_style.language == DocLanguage.RUSSIAN else "Write in English"

        prompt = f"""
        –°–æ–∑–¥–∞–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –≤ Markdown.

        –ö–û–ú–ü–û–ù–ï–ù–¢–´:
        {json.dumps(components[:50], indent=2, ensure_ascii=False)}

        –ü–ê–¢–¢–ï–†–ù–´:
        {json.dumps(patterns, indent=2, ensure_ascii=False)}

        –°–¢–†–£–ö–¢–£–†–ê –§–ê–ô–õ–û–í:
        {json.dumps(file_structure[:20], indent=2, ensure_ascii=False)}

        –¢–û–ß–ö–ò –ò–ù–¢–ï–ì–†–ê–¶–ò–ò:
        {json.dumps(integration_points[:10], indent=2, ensure_ascii=False)}

        –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
        {json.dumps(recommendations, indent=2, ensure_ascii=False)}

        –¢–ï–•–ù–û–õ–û–ì–ò–ò:
        –Ø–∑—ã–∫: {tech_stack.primary_language}
        –§—Ä–µ–π–º–≤–æ—Ä–∫–∏: {', '.join(tech_stack.frameworks) if tech_stack.frameworks else 'N/A'}
        –ü–∞—Ç—Ç–µ—Ä–Ω—ã: {', '.join(tech_stack.architecture_patterns) if tech_stack.architecture_patterns else 'N/A'}
        –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
        {lang_note}
        –°–µ–∫—Ü–∏–∏:
        –û–±–∑–æ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏ –∏—Ö –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏
        –°–ª–æ–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞
        –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏
        –î–∏–∞–≥—Ä–∞–º–º—ã
        –†–µ—à–µ–Ω–∏—è –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è (ADR)
        –î–æ–±–∞–≤—å Table of Contents
        –î–ò–ê–ì–†–ê–ú–ú–´ (–≤–∫–ª—é—á–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç):
        {diagrams_section if diagrams_section else "–î–∏–∞–≥—Ä–∞–º–º—ã –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã"}

        –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ Markdown –∫–æ–Ω—Ç–µ–Ω—Ç.
        """
    content = await call_llm(prompt, max_tokens=100000, step="architecture_docs_generation")

    content = re.sub(r'^```(?:markdown)?\n?', '', content)
    content = re.sub(r'\n?```$', '', content)
    content = content.strip()

    return DocFile(
        path="docs/architecture.md",
        content=content,
        doc_type=DocType.ARCHITECTURE,
        format=doc_style.format,
        description="Architecture Documentation",
        action="create",
        word_count=count_words(content)
    )

# ============================================================================
# CHANGELOG GENERATION
# ============================================================================
async def generate_changelog(
task: str,
code_files: List[Dict[str, Any]],
review_result: Dict[str, Any],
existing_changelog: Optional[str] = None
) -> Tuple[DocFile, ChangelogVersion]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç CHANGELOG entry
    """

    # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö
    files_info = []
    for f in code_files[:20]:
        files_info.append({
            "path": f.get("path", ""),
            "action": f.get("action", "create"),
            "description": f.get("description", "")
        })

    quality_score = review_result.get("quality_score", 0) if review_result else 0

    prompt = f"""
–°–æ–∑–¥–∞–π –∑–∞–ø–∏—Å—å –¥–ª—è CHANGELOG –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π.

–ó–ê–î–ê–ß–ê:
{task}

–ò–ó–ú–ï–ù–Å–ù–ù–´–ï –§–ê–ô–õ–´:
{json.dumps(files_info, indent=2, ensure_ascii=False)}

–ö–ê–ß–ï–°–¢–í–û –ö–û–î–ê:
{quality_score}/10

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (JSON):
{{
"version": "X.Y.Z",
"entries": [
{{
"change_type": "added/changed/fixed/deprecated/removed/security",
"description": "–û–ø–∏—Å–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è",
"component": "–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –∫–∞–∫–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –∑–∞—Ç—Ä–æ–Ω—É—Ç"
}}
]
}}

–ü–†–ê–í–ò–õ–ê:
added: –Ω–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å

changed: –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏

fixed: –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±–∞–≥–æ–≤

deprecated: —Å–∫–æ—Ä–æ –±—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–æ

removed: —É–¥–∞–ª—ë–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å

security: –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
"""

    response = await call_llm(prompt, step="changelog_generation")
    parsed = parse_json_response(response)

    version = "0.1.0"
    entries = []

    if parsed:
        version = parsed.get("version", "0.1.0")
        for entry_data in parsed.get("entries", []):
            try:
                change_type = ChangeType(entry_data.get("change_type", "added"))
            except ValueError:
                change_type = ChangeType.ADDED
            entries.append(ChangelogEntry(
                change_type=change_type,
                description=entry_data.get("description", ""),
                component=entry_data.get("component")
            ))

    if not entries:
        entries.append(ChangelogEntry(
        change_type=ChangeType.ADDED,
        description=task[:100],
        component=None
    ))

    changelog_version = ChangelogVersion(
        version=version,
        entries=entries
    )

    content = format_changelog_markdown(changelog_version, existing_changelog)

    return DocFile(
        path="CHANGELOG.md",
        content=content,
        doc_type=DocType.CHANGELOG,
        format=DocFormat.MARKDOWN,
        description="Changelog",
        action="update" if existing_changelog else "create",
        word_count=count_words(content)
    ), changelog_version

def format_changelog_markdown(
version: ChangelogVersion,
existing_changelog: Optional[str] = None
) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç CHANGELOG –≤ Markdown
    """
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º entries –ø–æ —Ç–∏–ø—É
    by_type = {}
    for entry in version.entries:
        t = entry.change_type.value.capitalize()
        if t not in by_type:
            by_type[t] = []
        by_type[t].append(entry)

    # –§–æ—Ä–º–∏—Ä—É–µ–º –Ω–æ–≤—É—é —Å–µ–∫—Ü–∏—é
    new_section = f"## [{version.version}] - {version.date}\n\n"

    type_order = ["Added", "Changed", "Deprecated", "Removed", "Fixed", "Security"]

    for change_type in type_order:
        if change_type in by_type:
            new_section += f"### {change_type}\n\n"
            for entry in by_type[change_type]:
                component = f"**{entry.component}**: " if entry.component else ""
                new_section += f"- {component}{entry.description}\n"
            new_section += "\n"

    # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π CHANGELOG - –¥–æ–±–∞–≤–ª—è–µ–º –≤ –Ω–∞—á–∞–ª–æ
    if existing_changelog:
        # –ò—â–µ–º –º–µ—Å—Ç–æ –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        header_match = re.search(r'^#\s+Changelog.*?\n', existing_changelog, re.IGNORECASE)
        
        if header_match:
            header_end = header_match.end()
            content = (
                existing_changelog[:header_end] + 
                "\n" + new_section + 
                existing_changelog[header_end:]
            )
        else:
            content = f"# Changelog\n\n{new_section}\n{existing_changelog}"
    else:
        content = f"""# Changelog
    All notable changes to this project will be documented in this file.

    The format is based on Keep a Changelog,
    and this project adheres to Semantic Versioning.

    {new_section}"""

    return content

# ============================================================================
# CONTRIBUTING GUIDE
# ============================================================================

async def generate_contributing_guide(
tech_stack: TechStack,
doc_style: DocStyle
) -> DocFile:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç CONTRIBUTING.md
    """

    lang_note = "–ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ" if doc_style.language == DocLanguage.RUSSIAN else "Write in English"

    prompt = f"""
    –°–æ–∑–¥–∞–π CONTRIBUTING.md –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞.

    –¢–ï–•–ù–û–õ–û–ì–ò–ò:
    –Ø–∑—ã–∫: {tech_stack.primary_language}
    –§—Ä–µ–π–º–≤–æ—Ä–∫–∏: {', '.join(tech_stack.frameworks) if tech_stack.frameworks else 'N/A'}
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: {', '.join(tech_stack.testing_frameworks) if tech_stack.testing_frameworks else 'N/A'}
    Package Manager: {', '.join(tech_stack.package_managers) if tech_stack.package_managers else 'N/A'}
    –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
    {lang_note}
    –°–µ–∫—Ü–∏–∏:
    –ö–∞–∫ –Ω–∞—á–∞—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É
    –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    Code style guidelines
    –ü—Ä–æ—Ü–µ—Å—Å —Å–æ–∑–¥–∞–Ω–∏—è PR
    –ü—Ä–∞–≤–∏–ª–∞ –∫–æ–º–º–∏—Ç–æ–≤ (Conventional Commits)
    –ü—Ä–æ—Ü–µ—Å—Å —Ä–µ–≤—å—é
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –¥–ª—è —Å—Ç–µ–∫–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π
    –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ Markdown –∫–æ–Ω—Ç–µ–Ω—Ç.
    """

    content = await call_llm(prompt, max_tokens=100000, step="contributing_guide_generation")

    content = re.sub(r'^```(?:markdown)?\n?', '', content)
    content = re.sub(r'\n?```$', '', content)
    content = content.strip()

    return DocFile(
        path="CONTRIBUTING.md",
        content=content,
        doc_type=DocType.CONTRIBUTING,
        format=doc_style.format,
        description="Contributing Guide",
        action="create",
        word_count=count_words(content)
    )

# ============================================================================
# MAIN DOCUMENTATION GENERATION
# ============================================================================
async def generate_documentation(
task: str,
code_files: List[Dict[str, Any]],
architecture: Dict[str, Any],
review_result: Dict[str, Any],
tech_stack: TechStack,
repo_context: Dict[str, Any],
doc_types: List[DocType]
) -> List[DocFile]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—Å—é –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
    """

    files = []

    # 1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∏–ª—å
    logger.info("Analyzing documentation style...")
    doc_style = await analyze_doc_style(repo_context)

    # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã
    key_files = repo_context.get("key_files", {})
    existing_readme = key_files.get("README.md")
    existing_changelog = key_files.get("CHANGELOG.md")

    # 2. README
    if DocType.README in doc_types:
        logger.info("Generating README...")
        readme = await generate_readme(
            task=task,
            code_files=code_files,
            architecture=architecture,
            tech_stack=tech_stack,
            doc_style=doc_style,
            existing_readme=existing_readme
        )
        files.append(readme)

    # 3. API Documentation
    if DocType.API in doc_types:
        logger.info("Generating API documentation...")
        endpoints = await extract_api_endpoints(code_files, tech_stack)
        api_doc = await generate_api_documentation(endpoints, tech_stack, doc_style)
        if api_doc:
            files.append(api_doc)

    # 4. Code Documentation
    if DocType.CODE in doc_types:
        logger.info("Generating code documentation...")
        code_doc = await generate_code_documentation(
            code_files, architecture, tech_stack, doc_style
        )
        files.append(code_doc)

    # 5. Architecture Documentation
    if DocType.ARCHITECTURE in doc_types:
        logger.info("Generating architecture documentation...")
        arch_doc = await generate_architecture_documentation(
            architecture, tech_stack, doc_style
        )
        if arch_doc:
            files.append(arch_doc)

    # 6. CHANGELOG
    if DocType.CHANGELOG in doc_types:
        logger.info("Generating CHANGELOG...")
        changelog_file, _ = await generate_changelog(
            task, code_files, review_result, existing_changelog
        )
        files.append(changelog_file)

    # 7. CONTRIBUTING
    if DocType.CONTRIBUTING in doc_types:
        logger.info("Generating CONTRIBUTING guide...")
        contributing = await generate_contributing_guide(tech_stack, doc_style)
        files.append(contributing)

    return files

# ============================================================================
# MAIN ENDPOINT
# ============================================================================
@app.post("/process", response_model=DocumentationResponse)
async def process_documentation(request: DocumentationRequest):
    """
    –û—Å–Ω–æ–≤–Ω–æ–π endpoint –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.

    –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞–∫–∏–µ —Ç–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å
    –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    """

    start_time = time.time()
    task_id = str(uuid.uuid4())

    try:
        data = request.data
        
        logger.info(f"[{task_id[:8]}] Starting documentation generation: {request.task[:100]}")
        logger.info(f"[{task_id[:8]}] Received data keys: {list(data.keys())}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        code_data = data.get("code", {})
        code_files = code_data.get("files", [])
        if not code_files and "files" in data:
            code_files = data["files"]
        
        logger.info(f"[{task_id[:8]}] Code files count: {len(code_files)}")
        
        architecture = data.get("architecture", {})
        logger.info(f"[{task_id[:8]}] Architecture: components={len(architecture.get('components', []))}, "
                f"patterns={len(architecture.get('patterns', []))}")
        
        review_result = data.get("review", {})
        logger.info(f"[{task_id[:8]}] Review: present={bool(review_result)}, "
                f"score={review_result.get('quality_score', 'N/A')}")
        
        tech_stack_data = data.get("tech_stack", {})
        tech_stack = TechStack(**tech_stack_data) if tech_stack_data else TechStack()
        logger.info(f"[{task_id[:8]}] Tech stack: {tech_stack.primary_language}, "
                f"frameworks={tech_stack.frameworks}")
        
        repo_context = data.get("repo_context", {})
        logger.info(f"[{task_id[:8]}] Repo context: key_files={len(repo_context.get('key_files', {}))}")
        
        # =======================================================================
        # –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –¢–ò–ü–û–í –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–ò
        # =======================================================================
        logger.info(f"[{task_id[:8]}] Auto-determining documentation types...")
        
        doc_types = await determine_doc_types(
            task=request.task,
            code_files=code_files,
            architecture=architecture,
            review_result=review_result,
            repo_context=repo_context,
            tech_stack=tech_stack
        )
        
        logger.info(f"[{task_id[:8]}] Will generate: {[dt.value for dt in doc_types]}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
        files = await generate_documentation(
            task=request.task,
            code_files=code_files,
            architecture=architecture,
            review_result=review_result,
            tech_stack=tech_stack,
            repo_context=repo_context,
            doc_types=doc_types
        )
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∏–ª—å –¥–ª—è –æ—Ç–≤–µ—Ç–∞
        doc_style = await analyze_doc_style(repo_context)
        
        duration = time.time() - start_time
        total_words = sum(f.word_count for f in files)
        
        logger.info(f"[{task_id[:8]}] Documentation generated in {duration:.1f}s, "
                f"files: {len(files)}, words: {total_words}")
        
        return DocumentationResponse(
            task_id=task_id,
            status="success" if files else "error",
            files=files,
            doc_style=doc_style,
            sections_created=[f.doc_type.value for f in files],
            total_files=len(files),
            total_words=total_words,
            duration_seconds=duration
        )

    except Exception as e:
        logger.exception(f"[{task_id[:8]}] Documentation error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# ADDITIONAL ENDPOINTS
# ============================================================================
@app.post("/readme")
async def generate_readme_only(request: Dict[str, Any]):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ README
    """

    task = request.get("task", "")
    code_files = request.get("code_files", [])
    architecture = request.get("architecture", {})
    tech_stack_data = request.get("tech_stack", {})
    tech_stack = TechStack(**tech_stack_data) if tech_stack_data else TechStack()
    existing_readme = request.get("existing_readme")

    doc_style = DocStyle()

    readme = await generate_readme(
        task=task,
        code_files=code_files,
        architecture=architecture,
        tech_stack=tech_stack,
        doc_style=doc_style,
        existing_readme=existing_readme
    )

    return readme.dict()

@app.post("/api-docs")
async def generate_api_docs_only(request: Dict[str, Any]):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
    """

    code_files = request.get("code_files", [])
    tech_stack_data = request.get("tech_stack", {})
    tech_stack = TechStack(**tech_stack_data) if tech_stack_data else TechStack()

    endpoints = await extract_api_endpoints(code_files, tech_stack)
    doc_style = DocStyle()

    api_doc = await generate_api_documentation(endpoints, tech_stack, doc_style)

    return {
        "file": api_doc.dict() if api_doc else None,
        "endpoints_found": len(endpoints)
    }

@app.post("/changelog")
async def generate_changelog_only(request: Dict[str, Any]):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ CHANGELOG entry
    """
    task = request.get("task", "")
    code_files = request.get("code_files", [])
    review_result = request.get("review_result", {})
    existing_changelog = request.get("existing_changelog")

    changelog_file, changelog_version = await generate_changelog(
        task, code_files, review_result, existing_changelog
    )

    return {
        "file": changelog_file.dict(),
        "version": changelog_version.dict()
    }

@app.get("/health")
async def health_check():
    """Health check"""
    return {
    "status": "healthy",
    "service": "documentation",
    "version": "2.1.0",
    "timestamp": datetime.now().isoformat(),
    "features": {
    "auto_doc_types": True,
    "supported_types": [dt.value for dt in DocType]
    }
    }

@app.get("/")
async def root():
    """Root endpoint"""
    return {
    "service": "Documentation Agent",
    "version": "2.1.0",
    "description": "–ê–≥–µ–Ω—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–¥–∞ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã",
    "auto_detection": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞–∫–∏–µ —Ç–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å",
    "doc_types": [dt.value for dt in DocType],
    "receives_from": ["code_writer", "architect", "code_reviewer"],
    "outputs": [
    "README.md",
    "docs/api.md",
    "docs/code-reference.md",
    "docs/architecture.md",
    "CHANGELOG.md",
    "CONTRIBUTING.md"
    ],
    "endpoints": {
    "process": "POST /process - –ø–æ–ª–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (–∞–≤—Ç–æ-–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤)",
    "readme": "POST /readme - —Ç–æ–ª—å–∫–æ README",
    "api_docs": "POST /api-docs - —Ç–æ–ª—å–∫–æ API docs",
    "changelog": "POST /changelog - —Ç–æ–ª—å–∫–æ CHANGELOG",
    "health": "GET /health"
    }
    }

# ============================================================================
# MAIN
# ============================================================================
if __name__ == "__main__":
    uvicorn.run(
    "server:app",
    host="0.0.0.0",
    port=8000,
    reload=True,
    log_level="info"
    )